{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def label_regex (x) :\n",
    "    x = str(x)\n",
    "    x = re.sub('중성화수술','Neutral', x)\n",
    "    x = re.sub('중성화 수술','Neutral', x)\n",
    "    x = re.sub('중성화','Neutral', x)\n",
    "    x = re.sub('여아중성화','Neutral', x)\n",
    "    x = re.sub('남아중성화','Neutral', x)\n",
    "    x = re.sub('여아Neutral','Neutral', x)#여아Neutral\n",
    "    x = re.sub('남아Neutral','Neutral', x)\n",
    "    x = re.sub('여중','Neutral', x)\n",
    "    x = re.sub('남중','Neutral', x)\n",
    "\n",
    "    x = re.sub('[건강][가-힣 ()a-zA-Z,0-9\\/]+','test', x)\n",
    "\n",
    "    x = re.sub('X','normal', x)\n",
    "    x = re.sub('x','normal', x)\n",
    "    x = re.sub('이상없음','normal', x)\n",
    "    x = re.sub('XX','normal', x)\n",
    "    x = re.sub('-','normal', x)\n",
    "    x = re.sub('normalnormal','normal', x)\n",
    "    x = re.sub('정상','normal', x)\n",
    "    x = re.sub('없음','normal', x)\n",
    "\n",
    "    x = re.sub('스켈링','스케일링', x)\n",
    "\n",
    "    x = re.sub('[.]0','', x)\n",
    "    x = re.sub('[ ]+','', x)\n",
    "\n",
    "    x = re.sub('[(].+[)]', '', x)\n",
    "    return x\n",
    "\n",
    "# df['진단코드'] = label_regex(df['진단코드'])\n",
    "def my_strip_split (df) :\n",
    "    for column in ['보호자명', '환자명', '생일', '검사일', '병원'] :\n",
    "        df[column] = df[column].apply(lambda x : str(x).strip())\n",
    "    for column in ['생일', '검사일'] :\n",
    "        df[column] = df[column].apply(lambda x : x.split(' ')[0])\n",
    "    for column in ['병원'] :\n",
    "        df[column] = df[column].apply(lambda x : x.split('_')[0])\n",
    "    return df\n",
    "\n",
    "def csv_concat(data_path) :\n",
    "\n",
    "    dfall = []\n",
    "    #temp = [x.decode('euc-kr') for x in os.listdir(data_path)]\n",
    "    # print(os.listdir(data_path))\n",
    "    dir_list = [data_path+ '/' + d for d in os.listdir(data_path) if os.path.isdir(data_path + '/' + d)]\n",
    "#    print(dir_list)\n",
    "    for d in dir_list :\n",
    "        print(d)\n",
    "#         files = [f for f in os.listdir(d) if f.endswith('코드적용.xlsx')]\n",
    "        files = [f for f in os.listdir(d)]\n",
    "        print(files)\n",
    "        for f in files :\n",
    "#            f = d + '/' + f\n",
    "            print(f)\n",
    "            df = pd.read_excel(d + '/' + f)\n",
    "            if f.startswith('D01') :\n",
    "                df = pd.read_excel(d + '/' + f, 'Sheet1')\n",
    "#             print(df.columns)\n",
    "            df['병원'] = d.split('/')[1]\n",
    "            df1 = df[['보호자명', '환자명', '품종코드','생일', '성별코드', '검사일', '병원']]\n",
    "            if '주진단명\\n코드 1' in df.columns :\n",
    "                temp = df[['주진단명\\n코드 1','기타진단명\\n코드 2', '기타진단명\\n코드3', '기타진단명코드 4', '기타진단명코드 5']]\n",
    "                df1['진단코드'] = temp[temp.columns[:]].apply(\n",
    "                    lambda x: ','.join(x.dropna().astype(str)),\n",
    "                    axis=1)\n",
    "                df1.rename(columns ={'주진단명\\n코드 1':'주진단코드'}, inplace=True)\n",
    "\n",
    "            elif '주진단명코드 1' in df.columns :\n",
    "                temp = df[['주진단명코드 1', '기타진단명코드 2', '기타진단명코드3', '기타진단명코드 4', '기타진단명코드 5']]\n",
    "                df1['진단코드'] = temp[temp.columns[:]].apply(\n",
    "                    lambda x: ','.join(x.dropna().astype(str)),\n",
    "                    axis=1)\n",
    "                df1.rename(columns ={'주진단명코드 1':'주진단코드'}, inplace=True)\n",
    "            elif '주진단명코드\\n(단일항목기재)' in df.columns :\n",
    "                temp = df[['주진단명코드\\n(단일항목기재)', '기타진단명코드\\n(복수기재가능)']]\n",
    "                df1['진단코드'] = temp[temp.columns[:]].apply(\n",
    "                    lambda x: ','.join(x.dropna().astype(str)),\n",
    "                    axis=1)\n",
    "                df1.rename(columns ={'주진단명코드\\n(단일항목기재)':'주진단코드'}, inplace=True)\n",
    "            df1['SE'] = df['SE']\n",
    "            dfall.append(df1)\n",
    "    df = pd.concat(dfall, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def make_num_label(df) :\n",
    "    df['Num_Label'] = ''\n",
    "    df.Num_Label = df['진단코드'].apply(lambda x : len(str(x).split(',')))\n",
    "    return df\n",
    "\n",
    "#not used\n",
    "def make_label_dict(df, Num_class=50) :\n",
    "    for labels in df['진단코드'] :\n",
    "        labels = str(labels).split(',')\n",
    "        for label in labels:\n",
    "            la = str(label).split('/')\n",
    "            for l in la :\n",
    "                l = label_regex(l)\n",
    "                if l in label_dict :\n",
    "                    label_dict[l] += 1\n",
    "                else :\n",
    "                    label_dict[l] = 1\n",
    "    label_sorted = sorted(label_dict.items(), key=(lambda x : x[1]), reverse=True)\n",
    "    label_sorted_ = label_sorted[0:Num_class]\n",
    "    label_sorted_ = [label for label, number in label_sorted_]\n",
    "\n",
    "def make_label_col(df, label_list) :\n",
    "    for index, labels in zip(df.index, df['진단코드']) :\n",
    "        labels = str(labels).split(',')\n",
    "        for label in labels :\n",
    "            la = str(label).split('/')\n",
    "            for l in la :\n",
    "                l = label_regex(l)\n",
    "                if l in label_list :\n",
    "                    df.loc[index, l] = 1\n",
    "                else :\n",
    "                    df.loc[index, l] = 0\n",
    "    return df\n",
    "\n",
    "def set_label(x, label_list) :\n",
    "    if x in label_list :\n",
    "        return x\n",
    "    else :\n",
    "        return 'others'\n",
    "\n",
    "def mak_label_id(df) :\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df.label)\n",
    "    label_id = encoder.transform(df.label)\n",
    "    df['label_id'] = label_id\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_set, saved_dir, num=50):\n",
    "\n",
    "    df = csv_concat(data_set)\n",
    "    print('total df shape : ',df.shape)\n",
    "    df = df.dropna(subset=['SE']).reset_index(drop=True)\n",
    "    print('drop SE shape : ',df.shape)\n",
    "    df = my_strip_split(df)\n",
    "    df['진단코드'] = df['진단코드'].apply(lambda x : np.nan if str(x).strip(' \\t') == '' else x)\n",
    "    df = df.dropna(subset=['진단코드']).reset_index(drop=True)\n",
    "    print('drop diagnoise code shape : ',df.shape)\n",
    "    df['진단코드'] = df['진단코드'].apply(lambda x : label_regex(x))\n",
    "    df['label'] = df['진단코드'].apply(lambda x : str(x).split(',')[0])\n",
    "#    print(df.label.value_counts())\n",
    "\n",
    "    labels = df.label.value_counts()[0:num].to_dict()\n",
    "    label_list = [label for label in labels]\n",
    "\n",
    "    df.label = df.label.apply(lambda x : set_label(str(x), label_list))\n",
    "\n",
    "    label_dict = {}\n",
    "    print(df.columns)\n",
    "#    for index, label in zip(range(len(df.columns[1:])), df.columns[1:]) :\n",
    "    for index, label in zip(range(len(label_list)), label_list) :\n",
    "        label_dict[label] = index\n",
    "    label_dict['others'] = len(label_list)\n",
    "\n",
    "    df['label_id'] = df['label'].apply(lambda x : label_dict[str(x)])\n",
    "\n",
    "    if not (os.path.isdir(saved_dir)) :\n",
    "        os.makedirs(saved_dir)\n",
    "\n",
    "    df.to_csv(saved_dir + '/' + 'df.csv', index=None)\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size = 0.1, random_state=100)\n",
    "    print('train_len {} test_len {}'.format(len(train_df), len(test_df)))\n",
    "    train_df.to_csv(saved_dir + '/' + 'train.csv', index=None)\n",
    "    test_df.to_csv(saved_dir + '/' + 'test.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/D\n",
      "['D01.xlsx', 'D02.xlsx']\n",
      "D01.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon/bert/bert/bert-env/lib/python3.6/site-packages/ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/beomgon/bert/bert/bert-env/lib/python3.6/site-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/beomgon/bert/bert/bert-env/lib/python3.6/site-packages/ipykernel_launcher.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D02.xlsx\n",
      "data/C\n",
      "['C02.xlsx', 'C01.xlsx']\n",
      "C02.xlsx\n",
      "C01.xlsx\n",
      "data/A\n",
      "['A02.xlsx', 'A01.xlsx']\n",
      "A02.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon/bert/bert/bert-env/lib/python3.6/site-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A01.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon/bert/bert/bert-env/lib/python3.6/site-packages/ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/F\n",
      "['F01.xlsx']\n",
      "F01.xlsx\n",
      "data/B\n",
      "['B01.xlsx', 'B02.xlsx']\n",
      "B01.xlsx\n",
      "B02.xlsx\n",
      "data/E\n",
      "['E01.xlsx']\n",
      "E01.xlsx\n",
      "data/G\n",
      "['G01.xlsx']\n",
      "G01.xlsx\n",
      "total df shape :  (16459, 9)\n",
      "drop SE shape :  (14992, 9)\n",
      "drop diagnoise code shape :  (13366, 9)\n",
      "Index(['보호자명', '환자명', '품종코드', '생일', '성별코드', '검사일', '병원', '진단코드', 'SE',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "train_len 12029 test_len 1337\n"
     ]
    }
   ],
   "source": [
    "main('data', 'files2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import os\n",
    "# import sklearn\n",
    "# import argparse\n",
    "# import tensorflow as tf\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../ALBERT')\n",
    "# import time\n",
    "# import modeling\n",
    "# import tokenization\n",
    "# import optimization\n",
    "\n",
    "\n",
    "\n",
    "# def get_class_num(f) :\n",
    "#     df = pd.read_csv(f)\n",
    "#     return len(df.label_id.value_counts())\n",
    "\n",
    "# def process_input(file, mode, tokenizer):\n",
    "# #    df_data = pd.read_csv(file, names = ['SE','label_id'], skiprows =1)\n",
    "#     df_data = pd.read_csv(file)\n",
    "#     df_data = df_data[['SE', 'label_id']]\n",
    "#     df_data.dropna(subset=['SE'], inplace=True)\n",
    "#     print('mode ', mode)\n",
    "\n",
    "#     if ((mode == tf.estimator.ModeKeys.TRAIN)|(mode == tf.estimator.ModeKeys.EVAL)):\n",
    "\n",
    "#         df_data['text_tokens'] = df_data.SE.apply(tokenizer.tokenize)\n",
    "#         print(df_data['text_tokens'])\n",
    "# #         print(([df_data.text_tokens]))\n",
    "# #         print(len(df_data.text_tokens))\n",
    "\n",
    "# #         input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(list(df_data.text_tokens))],\n",
    "# #                           maxlen=args.max_seq_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "#         input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in df_data['text_tokens']],\n",
    "#                           maxlen=args.max_seq_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "#         labels = df_data.label_id\n",
    "#         # create the mask to ignore the padded elements in the sequences.\n",
    "#         input_mask = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "#         return input_ids, labels, input_mask\n",
    "\n",
    "#     else: # Predict\n",
    "#         df_data['text_tokens'] = df_data.SE.apply(tokenizer.tokenize)\n",
    "#         input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in df_data['text_tokens']],\n",
    "#                           maxlen=args.max_seq_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "#         # create the mask to ignore the padded elements in the sequences.\n",
    "#         input_mask = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "#         #print(df_data['text_tokens'])\n",
    "#         #print(input_ids)\n",
    "#         return input_ids, None, input_mask\n",
    "\n",
    "# def input_fn_builder(file, seq_length, drop_remainder, tokenizer):\n",
    "#     \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "#     def input_fn(mode, params):\n",
    "#         print(\"mode:\", mode)\n",
    "\n",
    "#         input_ids, labels, input_mask = process_input(file, mode, tokenizer)\n",
    "#         print(\"input_ids : \", input_ids[0:2] )\n",
    "# #        print(\"labels : \", labels )\n",
    "#         print(\"input_mask : \", input_mask[0:2] )\n",
    "\n",
    "#         \"\"\"The actual input function.\"\"\"\n",
    "#         batch_size = params[\"batch_size\"]\n",
    "\n",
    "#         num_examples = len(input_ids)\n",
    "#         print(\"******num_examples******\", num_examples)\n",
    "\n",
    "#         # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "#         # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "#         # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "#         if ((mode == tf.estimator.ModeKeys.TRAIN)|(mode == tf.estimator.ModeKeys.EVAL)):\n",
    "#             d = tf.data.Dataset.from_tensor_slices(\n",
    "#                 ({\"input_ids\": tf.constant( input_ids, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "#                   \"input_mask\": tf.constant( input_mask, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "#                   \"segment_ids\": tf.zeros(shape=[num_examples, seq_length], dtype=tf.int32),}\n",
    "#                  ,tf.constant(labels, shape=[num_examples], dtype=tf.int32),))\n",
    "\n",
    "#             if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#                 d = d.repeat()\n",
    "#                 d = d.shuffle(buffer_size=100)\n",
    "\n",
    "#             d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "#             return d\n",
    "#         else: # Predict\n",
    "#             d = tf.data.Dataset.from_tensor_slices(\n",
    "#                 ({\"input_ids\": tf.constant( input_ids, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "#                   \"input_mask\": tf.constant( input_mask, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "#                   \"segment_ids\": tf.zeros(shape=[num_examples, seq_length], dtype=tf.int32),}\n",
    "#                  ,tf.zeros(shape=[num_examples], dtype=tf.int32),))\n",
    "\n",
    "#             d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "#             return d\n",
    "\n",
    "#     return input_fn\n",
    "\n",
    "# def create_model(albert_config, mode, input_ids, input_mask, segment_ids,\n",
    "#                                  labels, num_labels):\n",
    "#     \"\"\"Creates a classification model.\"\"\"\n",
    "#     is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "#     model = modeling.AlbertModel(\n",
    "#             config=albert_config,\n",
    "#             is_training=is_training,\n",
    "#             input_ids=input_ids,\n",
    "#             input_mask=input_mask,\n",
    "#             token_type_ids=segment_ids)\n",
    "\n",
    "#     #output_layer = model.get_sequence_output()\n",
    "#     output_layer = model.get_pooled_output()\n",
    "\n",
    "#     hidden_size = output_layer.shape[-1].value\n",
    "#     print(\"*************************num_labels***********\", num_labels)\n",
    "\n",
    "#     output_weight = tf.get_variable(\n",
    "#             \"output_weights\", [num_labels, hidden_size],\n",
    "#             initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "#     output_bias = tf.get_variable(\n",
    "#             \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "#     with tf.variable_scope(\"loss\"):\n",
    "#         if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#             output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "#         output_layer = tf.reshape(output_layer, [-1, hidden_size])\n",
    "#         logits = tf.matmul(output_layer, output_weight, transpose_b=True)\n",
    "#         logits = tf.nn.bias_add(logits, output_bias)\n",
    "#         #logits = tf.reshape(logits, [-1, args.max_seq_len, num_labels])\n",
    "#         logits = tf.reshape(logits, [-1, num_labels])\n",
    "#         probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "#         predict = tf.argmax(probabilities,axis=-1)\n",
    "#         if ((mode == tf.estimator.ModeKeys.TRAIN)|(mode == tf.estimator.ModeKeys.EVAL)):\n",
    "#             log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "#             one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "#             per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "#             loss = tf.reduce_sum(per_example_loss)\n",
    "#             return (loss, per_example_loss, logits,predict)\n",
    "#         else: # Predict\n",
    "#             return (None, None, logits,predict)\n",
    "\n",
    "\n",
    "# def model_fn_builder(albert_config, num_labels, init_checkpoint, learning_rate,\n",
    "#                      num_train_steps, num_warmup_steps, use_tpu,\n",
    "#                      hub_module=None, optimizer=\"adamw\"):\n",
    "#     \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "#     def model_fn(features, labels, mode, params):       # pylint: disable=unused-argument\n",
    "#         \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "#         tf.logging.info(\"*** Features ***\")\n",
    "#         for name in sorted(features.keys()):\n",
    "#             tf.logging.info(f\"  name = {name}, shape = {features[name].shape}\")\n",
    "\n",
    "#         input_ids = features[\"input_ids\"]\n",
    "#         input_mask = features[\"input_mask\"]\n",
    "#         segment_ids = features[\"segment_ids\"]\n",
    "#         label_ids = labels\n",
    "\n",
    "#         (total_loss, per_example_loss, logits, predictions) = \\\n",
    "#                 create_model(albert_config, mode, input_ids, input_mask,\n",
    "#                                          segment_ids, label_ids, num_labels)\n",
    "\n",
    "#         tvars = tf.trainable_variables()\n",
    "#         initialized_variable_names = {}\n",
    "#         scaffold_fn = None\n",
    "#         if init_checkpoint:\n",
    "#             (assignment_map, initialized_variable_names\n",
    "#             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "#             if use_tpu:\n",
    "#                 def tpu_scaffold():\n",
    "#                     tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "#                     return tf.train.Scaffold()\n",
    "\n",
    "#                 scaffold_fn = tpu_scaffold\n",
    "#             else:\n",
    "#                 print(\"********weights is initialize from checkpoint**********\")\n",
    "#                 tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "#         tf.logging.info(\"**** Trainable Variables ****\")\n",
    "#         for var in tvars:\n",
    "#             init_string = \"\"\n",
    "#             if var.name in initialized_variable_names:\n",
    "#                 init_string = \", *INIT_FROM_CKPT*\"\n",
    "#             tf.logging.info(f\"  name = {var.name}, shape = {var.shape}{init_string}\")\n",
    "\n",
    "#         output_spec = None\n",
    "#         if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "#             train_op = optimization.create_optimizer(\n",
    "#                     total_loss, learning_rate, num_train_steps, num_warmup_steps,\n",
    "#                     use_tpu, optimizer)\n",
    "\n",
    "#             output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "#                     mode=mode,\n",
    "#                     loss=total_loss,\n",
    "#                     train_op=train_op,\n",
    "#                     scaffold_fn=scaffold_fn)\n",
    "#         elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "#             def metric_fn(per_example_loss, label_ids, logits):\n",
    "#                 predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "#                 accuracy = tf.metrics.mean(tf.math.equal(label_ids,predictions))\n",
    "#                 loss = tf.metrics.mean(values=per_example_loss)\n",
    "#                 #\n",
    "#                 return {\n",
    "#                     \"eval_accuracy\":accuracy,\n",
    "#                     \"eval_loss\": loss,\n",
    "#                 }\n",
    "#             eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
    "#             output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "#                     mode=mode,\n",
    "#                     loss=total_loss,\n",
    "#                     eval_metrics=eval_metrics,\n",
    "#                     scaffold_fn=scaffold_fn)\n",
    "#         else: # Predict\n",
    "#             output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "#                     mode=mode,\n",
    "#                     predictions= predictions,\n",
    "#                     scaffold_fn=scaffold_fn)\n",
    "#         return output_spec\n",
    "#     return model_fn\n",
    "        \n",
    "    \n",
    "# def get_accuracy(predictions, file) :\n",
    "#     df_test = pd.read_csv(file)\n",
    "#     labels = list(df_test.label_id)\n",
    "\n",
    "#     right = 0\n",
    "#     wrong = 0\n",
    "\n",
    "#     for i in range(len(labels)) :\n",
    "#         if labels[i] == predictions[i] :\n",
    "#             right += 1\n",
    "#         else :\n",
    "#             wrong += 1\n",
    "#             print( \"Wrong label : {} predict {} \".format(labels[i], predictions[i]) )\n",
    "#     accuracy = right/(right+wrong)\n",
    "# #    print('accuracy ', accuracy)\n",
    "#     return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_main(args) :\n",
    "\n",
    "#     Num_class = get_class_num(args.train_files)\n",
    "#     print(\"Num Class : \", Num_class)\n",
    "#     print(args)\n",
    "\n",
    "#     config = modeling.AlbertConfig.from_json_file('albert_config.json')\n",
    "#     tokenizer = tokenization.FullTokenizer.from_scratch(vocab_file = args.vocab_files, do_lower_case=False, spm_model_file=None)\n",
    "\n",
    "#     print(\"fine tune dir\", args.finetune)\n",
    "#     USE_TPU = False\n",
    "# #     run_config = tf.contrib.tpu.RunConfig(model_dir=args.finetune,)\n",
    "#     run_config = tf.contrib.tpu.RunConfig(model_dir='finetune3',)\n",
    "    \n",
    "#     model_fn = model_fn_builder(\n",
    "#             albert_config=config,\n",
    "# #            num_labels=len(label_list),\n",
    "#             num_labels=Num_class,\n",
    "#             init_checkpoint=args.init_checkpoint,\n",
    "#             learning_rate=args.rate,\n",
    "#             num_train_steps=args.num_train_step,\n",
    "#             num_warmup_steps=args.warmup_step,\n",
    "#             use_tpu=USE_TPU)\n",
    "\n",
    "#     # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "#     # or GPU.\n",
    "#     estimator = tf.contrib.tpu.TPUEstimator(\n",
    "#             use_tpu=USE_TPU,\n",
    "#             model_fn=model_fn,\n",
    "#             config=run_config,\n",
    "#             train_batch_size=args.batchsize,\n",
    "#             eval_batch_size=args.batchsize,\n",
    "#             predict_batch_size=args.batchsize)\n",
    "\n",
    "# #     tf.logging.info(\"***** Running training *****\")\n",
    "# #     # import sys\n",
    "# #     start = time.time()\n",
    "\n",
    "# #     train_input_fn = input_fn_builder(\n",
    "# #             file = args.train_files,\n",
    "# #             tokenizer = tokenizer,\n",
    "# #             seq_length=args.max_seq_len,\n",
    "# #             drop_remainder=True)\n",
    "\n",
    "# #     estimator.train(input_fn=train_input_fn, max_steps=args.num_train_step)\n",
    "\n",
    "# #     print(\"training time :\", time.time() - start)\n",
    "\n",
    "\n",
    "# # #     tf.saved_model.save(model_fn, args.saved_path)\n",
    "\n",
    "# #     predict_input_fn = input_fn_builder(\n",
    "# #             file = args.test_files,\n",
    "# #             tokenizer = tokenizer,\n",
    "# #             seq_length=args.max_seq_len,\n",
    "# #             drop_remainder=False)\n",
    "\n",
    "\n",
    "# #     predictions = []\n",
    "# #     print('finetune3' + '/model.ckpt-' + str(args.num_train_step))\n",
    "# # #     for prediction in estimator.predict(input_fn=predict_input_fn, checkpoint_path=args.finetune + '/model.ckpt-' + str(args.num_train_step)):\n",
    "# #     for prediction in estimator.predict(input_fn=predict_input_fn, checkpoint_path='finetune3' + '/model.ckpt-' + str(args.num_train_step)):        \n",
    "# #         #print(\"*******prediction*********\", prediction)\n",
    "# #         predictions.append(prediction)\n",
    "\n",
    "# #     accuracy = get_accuracy(predictions, args.test_files)\n",
    "# #     print('test accuracy ', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-tr\", \"--train_files\", help=\"train csv file\", default=\"files2/train.csv\")\n",
    "# parser.add_argument(\"-te\", \"--test_files\", help=\"test csv file\", default=\"files2/test.csv\")\n",
    "# parser.add_argument(\"-c\", \"--config_files\", help=\"config file\", default=\"albert_config.json\")\n",
    "# parser.add_argument(\"-v\", \"--vocab_files\", help=\"vocab file\", default=\"vocab.txt\")\n",
    "# parser.add_argument(\"-s\", \"--saved_path\", help=\"saved path\", default=\"saved/\")\n",
    "# # parser.add_argument(\"-e\", \"--epochs\", help=\"epochs\", type=int, default=1)\n",
    "# parser.add_argument(\"-b\", \"--batchsize\", help=\"batchsize\", type=int, default=16)\n",
    "# parser.add_argument(\"-r\", \"--rate\", help=\"learning_rate\", type=float, default=0.0001)\n",
    "# parser.add_argument(\"-w\", \"--warmup_step\", help=\"warmup steps\", type=int, default=1500)\n",
    "# parser.add_argument(\"-n\", \"--num_train_step\", help=\"num train steps\", type=int, default=3000)\n",
    "# parser.add_argument(\"-i\", \"--init_checkpoint\", help=\"checkpoint init dir\", default=\"models/\")\n",
    "# parser.add_argument(\"-f\", \"--finetune\", help=\"fine tunning dir\", default=\"finetune3/\")\n",
    "# parser.add_argument(\"-m\", \"--max_seq_len\", help=\"max_seq_len\", type=int,  default=250)\n",
    "# #    parser.add_argument(\"-w\", \"--weights\", help=\"weights\", action=\"store_true\")\n",
    "# #    parser.add_argument(\"-t\", \"--trainable\", help=\"trainable\", action=\"store_true\")\n",
    "# #    parser.add_argument(\"-c\", \"--checkpoint\", help=\"checkpoint\")\n",
    "# args = parser.parse_args()\n",
    "# print(args)\n",
    "# args.finetune_dir = \"finetune3\"\n",
    "# print(args.finetune_dir)\n",
    "# train_main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('files2/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>보호자명</th>\n",
       "      <th>환자명</th>\n",
       "      <th>품종코드</th>\n",
       "      <th>생일</th>\n",
       "      <th>성별코드</th>\n",
       "      <th>검사일</th>\n",
       "      <th>병원</th>\n",
       "      <th>진단코드</th>\n",
       "      <th>SE</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>김소야*7</td>\n",
       "      <td>코코</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2001-05-03</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-07-04</td>\n",
       "      <td>D</td>\n",
       "      <td>2078,만성신질환,신장결석,방광염</td>\n",
       "      <td>S) 식욕부진, 기력저하, 눈    O)  - 2일동안 식욕이 거의 없어서 보호자분...</td>\n",
       "      <td>others</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이지은*6</td>\n",
       "      <td>힘찬이</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2007-06-04</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-04-26</td>\n",
       "      <td>D</td>\n",
       "      <td>2244,담낭점액종</td>\n",
       "      <td>S) 다음다뇨, 식욕부진    O)  - 최근들어 다음다뇨 증상과 2~3일전부터는 ...</td>\n",
       "      <td>2244</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>홍성미</td>\n",
       "      <td>비쥬</td>\n",
       "      <td>499.0</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>FS</td>\n",
       "      <td>2018-07-20</td>\n",
       "      <td>D</td>\n",
       "      <td>2082,만성신질환,부비동염</td>\n",
       "      <td>[오전 통화 by 관중]  - 일주일동안 밥안먹고, 활력저하됨  - 보호자님 복부촉...</td>\n",
       "      <td>2082</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>김래경</td>\n",
       "      <td>세순이</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2001-08-01</td>\n",
       "      <td>FS</td>\n",
       "      <td>2017-06-16</td>\n",
       "      <td>D</td>\n",
       "      <td>2087,단백뇨,쿠싱,빈혈</td>\n",
       "      <td>1. 쿠싱약 안먹고 와서 쿠싱검사는 28일에 하기로 함  2. 흉방에서 CVC 허탈...</td>\n",
       "      <td>2087</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>김지현*6</td>\n",
       "      <td>효</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2009-11-05</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>D</td>\n",
       "      <td>2081,방광결석</td>\n",
       "      <td>S) 건강검진 진행함    O)  1. 혈액검사  - 간수치 상승(ALP, ALT ...</td>\n",
       "      <td>2081</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>박수진</td>\n",
       "      <td>티키</td>\n",
       "      <td>598.0</td>\n",
       "      <td>2005-05-04</td>\n",
       "      <td>FS</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>D</td>\n",
       "      <td>2244,지방간,췌장염</td>\n",
       "      <td>538,300원 선결제 하셨습니다. - 송이    S) 식욕부진, 구토    O) ...</td>\n",
       "      <td>2244</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>서주현</td>\n",
       "      <td>돼지</td>\n",
       "      <td>130.0</td>\n",
       "      <td>2004-03-08</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>D</td>\n",
       "      <td>2231,방광결석,간비대</td>\n",
       "      <td>S) 체중감소, 건강검진    O)  - 혈액검사 : 빈혈 진행중(33%)  - 염...</td>\n",
       "      <td>others</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>오주영</td>\n",
       "      <td>달콩이</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2004-06-06</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-04-21</td>\n",
       "      <td>D</td>\n",
       "      <td>2177,갑기저,고혈압,방광염,쿠싱</td>\n",
       "      <td>667,060 미수금 발생 - 휘린     [한방진료]  -후지 근육 강화를 위한 ...</td>\n",
       "      <td>others</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>정여연*7</td>\n",
       "      <td>유자</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2009-10-28</td>\n",
       "      <td>FS</td>\n",
       "      <td>2018-02-11</td>\n",
       "      <td>D</td>\n",
       "      <td>2092</td>\n",
       "      <td>내추럴발란스캔 결제완료후 두고 가심  18.02.12일에 가져가실예정  효정    ...</td>\n",
       "      <td>2092</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>강보람</td>\n",
       "      <td>콩쥐</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2005-03-31</td>\n",
       "      <td>FS</td>\n",
       "      <td>2018-06-19</td>\n",
       "      <td>D</td>\n",
       "      <td>2196</td>\n",
       "      <td>S)  - 특이사항 없음.   - 잘 지냄.     O)  - 청진 시 NRF  -...</td>\n",
       "      <td>others</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    보호자명  환자명   품종코드          생일 성별코드         검사일 병원                 진단코드  \\\n",
       "0  김소야*7   코코   91.0  2001-05-03   MN  2018-07-04  D  2078,만성신질환,신장결석,방광염   \n",
       "1  이지은*6  힘찬이  119.0  2007-06-04   MN  2018-04-26  D           2244,담낭점액종   \n",
       "2    홍성미   비쥬  499.0  2004-01-01   FS  2018-07-20  D      2082,만성신질환,부비동염   \n",
       "3    김래경  세순이  107.0  2001-08-01   FS  2017-06-16  D       2087,단백뇨,쿠싱,빈혈   \n",
       "4  김지현*6    효  125.0  2009-11-05   MN  2018-02-28  D            2081,방광결석   \n",
       "5    박수진   티키  598.0  2005-05-04   FS  2017-10-08  D         2244,지방간,췌장염   \n",
       "6    서주현   돼지  130.0  2004-03-08   MN  2018-02-14  D        2231,방광결석,간비대   \n",
       "7    오주영  달콩이  128.0  2004-06-06   MN  2018-04-21  D  2177,갑기저,고혈압,방광염,쿠싱   \n",
       "8  정여연*7   유자   91.0  2009-10-28   FS  2018-02-11  D                 2092   \n",
       "9    강보람   콩쥐  125.0  2005-03-31   FS  2018-06-19  D                 2196   \n",
       "\n",
       "                                                  SE   label  label_id  \n",
       "0  S) 식욕부진, 기력저하, 눈    O)  - 2일동안 식욕이 거의 없어서 보호자분...  others        50  \n",
       "1  S) 다음다뇨, 식욕부진    O)  - 최근들어 다음다뇨 증상과 2~3일전부터는 ...    2244        26  \n",
       "2  [오전 통화 by 관중]  - 일주일동안 밥안먹고, 활력저하됨  - 보호자님 복부촉...    2082         4  \n",
       "3  1. 쿠싱약 안먹고 와서 쿠싱검사는 28일에 하기로 함  2. 흉방에서 CVC 허탈...    2087         6  \n",
       "4  S) 건강검진 진행함    O)  1. 혈액검사  - 간수치 상승(ALP, ALT ...    2081        34  \n",
       "5  538,300원 선결제 하셨습니다. - 송이    S) 식욕부진, 구토    O) ...    2244        26  \n",
       "6  S) 체중감소, 건강검진    O)  - 혈액검사 : 빈혈 진행중(33%)  - 염...  others        50  \n",
       "7  667,060 미수금 발생 - 휘린     [한방진료]  -후지 근육 강화를 위한 ...  others        50  \n",
       "8  내추럴발란스캔 결제완료후 두고 가심  18.02.12일에 가져가실예정  효정    ...    2092        10  \n",
       "9  S)  - 특이사항 없음.   - 잘 지냄.     O)  - 청진 시 NRF  -...  others        50  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# file_dir = 'files2'\n",
    "# df = pd.read_csv(file_dir + \"/df.csv\")\n",
    "# Num_Class = len(df.label.value_counts())\n",
    "# print(Num_Class)\n",
    "# df.head()\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal', 'Neutral', '2046', '2043', '2082', '2001', '2087', '2101', '2071', '2170', '2092', '2275', '2116', '2133', 'test', '2185', '2095', '2126', '2048']\n",
      "(9623, 11)\n"
     ]
    }
   ],
   "source": [
    "# df.label.value_counts()[1:20].to_dict()\n",
    "label_list_20 = []\n",
    "for key in df.label.value_counts()[1:20].to_dict():\n",
    "    label_list_20.append(key)\n",
    "print(label_list_20)\n",
    "df = df[df.loc[:,'label']!='others']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     보호자명  환자명   품종코드          생일 성별코드         검사일 병원             진단코드  \\\n",
      "1   이지은*6  힘찬이  119.0  2007-06-04   MN  2018-04-26  D       2244,담낭점액종   \n",
      "2     홍성미   비쥬  499.0  2004-01-01   FS  2018-07-20  D  2082,만성신질환,부비동염   \n",
      "3     김래경  세순이  107.0  2001-08-01   FS  2017-06-16  D   2087,단백뇨,쿠싱,빈혈   \n",
      "4   김지현*6    효  125.0  2009-11-05   MN  2018-02-28  D        2081,방광결석   \n",
      "5     박수진   티키  598.0  2005-05-04   FS  2017-10-08  D     2244,지방간,췌장염   \n",
      "8   정여연*7   유자   91.0  2009-10-28   FS  2018-02-11  D             2092   \n",
      "10    송혜림   쿠키  131.0  2011-03-01    F  2018-03-18  D        2091,기도허탈   \n",
      "11  문은진*7   다온  119.0  2011-04-18   FS  2018-02-17  D             2091   \n",
      "12    박해춘  박두식  598.0  2011-06-02   MN  2018-04-20  D          2087,빈혈   \n",
      "13  박예지*6   보리  119.0  2011-06-02   FS  2018-02-23  D             2170   \n",
      "\n",
      "                                                   SE   label  label_id  \n",
      "1   S) 다음다뇨, 식욕부진    O)  - 최근들어 다음다뇨 증상과 2~3일전부터는 ...  others        26  \n",
      "2   [오전 통화 by 관중]  - 일주일동안 밥안먹고, 활력저하됨  - 보호자님 복부촉...    2082         4  \n",
      "3   1. 쿠싱약 안먹고 와서 쿠싱검사는 28일에 하기로 함  2. 흉방에서 CVC 허탈...    2087         6  \n",
      "4   S) 건강검진 진행함    O)  1. 혈액검사  - 간수치 상승(ALP, ALT ...  others        34  \n",
      "5   538,300원 선결제 하셨습니다. - 송이    S) 식욕부진, 구토    O) ...  others        26  \n",
      "8   내추럴발란스캔 결제완료후 두고 가심  18.02.12일에 가져가실예정  효정    ...    2092        10  \n",
      "10  1.CC :  혈뇨    2.HPI   - 4~5일 전에 혈뇨 증상 처음 발견( C...  others        21  \n",
      "11  cc: 건강검진    S)  - 어제 자기 전 오심 한 것 외에 특이사항없음  - ...  others        21  \n",
      "12  S)  - 체중도 찌고 상태는 양호함.   - 설사는 지속되고 있음. 정상변-설사 ...    2087         6  \n",
      "13  cc: 건강검진    S)  - 꼬리부분, 복부 육아직 재 확인 요청  - 식탐 많...    2170         9  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>보호자명</th>\n",
       "      <th>환자명</th>\n",
       "      <th>품종코드</th>\n",
       "      <th>생일</th>\n",
       "      <th>성별코드</th>\n",
       "      <th>검사일</th>\n",
       "      <th>병원</th>\n",
       "      <th>진단코드</th>\n",
       "      <th>SE</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이지은*6</td>\n",
       "      <td>힘찬이</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2007-06-04</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-04-26</td>\n",
       "      <td>D</td>\n",
       "      <td>2244,담낭점액종</td>\n",
       "      <td>S) 다음다뇨, 식욕부진    O)  - 최근들어 다음다뇨 증상과 2~3일전부터는 ...</td>\n",
       "      <td>others</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>홍성미</td>\n",
       "      <td>비쥬</td>\n",
       "      <td>499.0</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>FS</td>\n",
       "      <td>2018-07-20</td>\n",
       "      <td>D</td>\n",
       "      <td>2082,만성신질환,부비동염</td>\n",
       "      <td>[오전 통화 by 관중]  - 일주일동안 밥안먹고, 활력저하됨  - 보호자님 복부촉...</td>\n",
       "      <td>2082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>김래경</td>\n",
       "      <td>세순이</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2001-08-01</td>\n",
       "      <td>FS</td>\n",
       "      <td>2017-06-16</td>\n",
       "      <td>D</td>\n",
       "      <td>2087,단백뇨,쿠싱,빈혈</td>\n",
       "      <td>1. 쿠싱약 안먹고 와서 쿠싱검사는 28일에 하기로 함  2. 흉방에서 CVC 허탈...</td>\n",
       "      <td>2087</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>김지현*6</td>\n",
       "      <td>효</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2009-11-05</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>D</td>\n",
       "      <td>2081,방광결석</td>\n",
       "      <td>S) 건강검진 진행함    O)  1. 혈액검사  - 간수치 상승(ALP, ALT ...</td>\n",
       "      <td>others</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>박수진</td>\n",
       "      <td>티키</td>\n",
       "      <td>598.0</td>\n",
       "      <td>2005-05-04</td>\n",
       "      <td>FS</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>D</td>\n",
       "      <td>2244,지방간,췌장염</td>\n",
       "      <td>538,300원 선결제 하셨습니다. - 송이    S) 식욕부진, 구토    O) ...</td>\n",
       "      <td>others</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>정여연*7</td>\n",
       "      <td>유자</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2009-10-28</td>\n",
       "      <td>FS</td>\n",
       "      <td>2018-02-11</td>\n",
       "      <td>D</td>\n",
       "      <td>2092</td>\n",
       "      <td>내추럴발란스캔 결제완료후 두고 가심  18.02.12일에 가져가실예정  효정    ...</td>\n",
       "      <td>2092</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>송혜림</td>\n",
       "      <td>쿠키</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>F</td>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>D</td>\n",
       "      <td>2091,기도허탈</td>\n",
       "      <td>1.CC :  혈뇨    2.HPI   - 4~5일 전에 혈뇨 증상 처음 발견( C...</td>\n",
       "      <td>others</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>문은진*7</td>\n",
       "      <td>다온</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2011-04-18</td>\n",
       "      <td>FS</td>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>D</td>\n",
       "      <td>2091</td>\n",
       "      <td>cc: 건강검진    S)  - 어제 자기 전 오심 한 것 외에 특이사항없음  - ...</td>\n",
       "      <td>others</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>박해춘</td>\n",
       "      <td>박두식</td>\n",
       "      <td>598.0</td>\n",
       "      <td>2011-06-02</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>D</td>\n",
       "      <td>2087,빈혈</td>\n",
       "      <td>S)  - 체중도 찌고 상태는 양호함.   - 설사는 지속되고 있음. 정상변-설사 ...</td>\n",
       "      <td>2087</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>박예지*6</td>\n",
       "      <td>보리</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2011-06-02</td>\n",
       "      <td>FS</td>\n",
       "      <td>2018-02-23</td>\n",
       "      <td>D</td>\n",
       "      <td>2170</td>\n",
       "      <td>cc: 건강검진    S)  - 꼬리부분, 복부 육아직 재 확인 요청  - 식탐 많...</td>\n",
       "      <td>2170</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     보호자명  환자명   품종코드          생일 성별코드         검사일 병원             진단코드  \\\n",
       "1   이지은*6  힘찬이  119.0  2007-06-04   MN  2018-04-26  D       2244,담낭점액종   \n",
       "2     홍성미   비쥬  499.0  2004-01-01   FS  2018-07-20  D  2082,만성신질환,부비동염   \n",
       "3     김래경  세순이  107.0  2001-08-01   FS  2017-06-16  D   2087,단백뇨,쿠싱,빈혈   \n",
       "4   김지현*6    효  125.0  2009-11-05   MN  2018-02-28  D        2081,방광결석   \n",
       "5     박수진   티키  598.0  2005-05-04   FS  2017-10-08  D     2244,지방간,췌장염   \n",
       "8   정여연*7   유자   91.0  2009-10-28   FS  2018-02-11  D             2092   \n",
       "10    송혜림   쿠키  131.0  2011-03-01    F  2018-03-18  D        2091,기도허탈   \n",
       "11  문은진*7   다온  119.0  2011-04-18   FS  2018-02-17  D             2091   \n",
       "12    박해춘  박두식  598.0  2011-06-02   MN  2018-04-20  D          2087,빈혈   \n",
       "13  박예지*6   보리  119.0  2011-06-02   FS  2018-02-23  D             2170   \n",
       "\n",
       "                                                   SE   label  label_id  \n",
       "1   S) 다음다뇨, 식욕부진    O)  - 최근들어 다음다뇨 증상과 2~3일전부터는 ...  others        18  \n",
       "2   [오전 통화 by 관중]  - 일주일동안 밥안먹고, 활력저하됨  - 보호자님 복부촉...    2082         5  \n",
       "3   1. 쿠싱약 안먹고 와서 쿠싱검사는 28일에 하기로 함  2. 흉방에서 CVC 허탈...    2087         6  \n",
       "4   S) 건강검진 진행함    O)  1. 혈액검사  - 간수치 상승(ALP, ALT ...  others        18  \n",
       "5   538,300원 선결제 하셨습니다. - 송이    S) 식욕부진, 구토    O) ...  others        18  \n",
       "8   내추럴발란스캔 결제완료후 두고 가심  18.02.12일에 가져가실예정  효정    ...    2092         7  \n",
       "10  1.CC :  혈뇨    2.HPI   - 4~5일 전에 혈뇨 증상 처음 발견( C...  others        18  \n",
       "11  cc: 건강검진    S)  - 어제 자기 전 오심 한 것 외에 특이사항없음  - ...  others        18  \n",
       "12  S)  - 체중도 찌고 상태는 양호함.   - 설사는 지속되고 있음. 정상변-설사 ...    2087         6  \n",
       "13  cc: 건강검진    S)  - 꼬리부분, 복부 육아직 재 확인 요청  - 식탐 많...    2170        13  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label = df.label.apply(lambda x : x if str(x) in label_list_20 else 'others' )\n",
    "df.label.value_counts()\n",
    "print(df.head(10))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df.label)\n",
    "X_train_encoded = encoder.transform(df.label)\n",
    "df['label_id'] =  X_train_encoded\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2001': 0, '2043': 1, '2046': 2, '2048': 3, '2071': 4, '2082': 5, '2087': 6, '2092': 7, '2095': 8, '2101': 9, '2116': 10, '2126': 11, '2133': 12, '2170': 13, '2185': 14, '2275': 15, 'Neutral': 16, 'normal': 17, 'others': 18, 'test': 19}\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(data['name'])\n",
    "le_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(le_name_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>보호자명</th>\n",
       "      <th>환자명</th>\n",
       "      <th>품종코드</th>\n",
       "      <th>생일</th>\n",
       "      <th>성별코드</th>\n",
       "      <th>검사일</th>\n",
       "      <th>병원</th>\n",
       "      <th>진단코드</th>\n",
       "      <th>SE</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13360</th>\n",
       "      <td>김옥</td>\n",
       "      <td>뽀뽀(R)</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2014-11-12</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>G</td>\n",
       "      <td>normal</td>\n",
       "      <td>리셉션(외래진료)    - 주증상 : 슬개골탈구 재발  - 증상발현일 :  처음 슬...</td>\n",
       "      <td>normal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13362</th>\n",
       "      <td>이순옥</td>\n",
       "      <td>다솔(R)</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>FS</td>\n",
       "      <td>2018-11-13</td>\n",
       "      <td>G</td>\n",
       "      <td>2082</td>\n",
       "      <td>&lt;오전관리사항&gt;    Food     - 식사종류 / 양 : 금식   - 식욕상태 ...</td>\n",
       "      <td>2082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13363</th>\n",
       "      <td>장윤경</td>\n",
       "      <td>아키</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2012-11-13</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-11-13</td>\n",
       "      <td>G</td>\n",
       "      <td>2046</td>\n",
       "      <td>S)  - 조금전 음식물 쓰레기 뒤져 닭뼈 다량 섭식.  - 구토했는데 조각 일부 ...</td>\n",
       "      <td>2046</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13364</th>\n",
       "      <td>박남선</td>\n",
       "      <td>이쁜이(R)</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2009-11-13</td>\n",
       "      <td>FS</td>\n",
       "      <td>2018-11-22</td>\n",
       "      <td>G</td>\n",
       "      <td>normal</td>\n",
       "      <td>우측 CCRL 수술/ 양측서혜탈장 수술    &lt;수술설명&gt;  - 연구개노장, 기관...</td>\n",
       "      <td>normal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13365</th>\n",
       "      <td>한순옥</td>\n",
       "      <td>루비</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2011-11-10</td>\n",
       "      <td>F</td>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>G</td>\n",
       "      <td>2274</td>\n",
       "      <td>동거견에 물려 안구돌출  보호자분 요청으로 병리학적 검사 진행  O)  혈액검사 상...</td>\n",
       "      <td>others</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      보호자명     환자명   품종코드          생일              성별코드         검사일 병원  \\\n",
       "13360   김옥   뽀뽀(R)  125.0  2014-11-12  MN                2018-11-12  G   \n",
       "13362  이순옥   다솔(R)  125.0  2013-11-12  FS                2018-11-13  G   \n",
       "13363  장윤경      아키  128.0  2012-11-13  MN                2018-11-13  G   \n",
       "13364  박남선  이쁜이(R)  131.0  2009-11-13  FS                2018-11-22  G   \n",
       "13365  한순옥      루비  125.0  2011-11-10  F                 2018-11-14  G   \n",
       "\n",
       "         진단코드                                                 SE   label  \\\n",
       "13360  normal  리셉션(외래진료)    - 주증상 : 슬개골탈구 재발  - 증상발현일 :  처음 슬...  normal   \n",
       "13362    2082  <오전관리사항>    Food     - 식사종류 / 양 : 금식   - 식욕상태 ...    2082   \n",
       "13363    2046  S)  - 조금전 음식물 쓰레기 뒤져 닭뼈 다량 섭식.  - 구토했는데 조각 일부 ...    2046   \n",
       "13364  normal    우측 CCRL 수술/ 양측서혜탈장 수술    <수술설명>  - 연구개노장, 기관...  normal   \n",
       "13365    2274  동거견에 물려 안구돌출  보호자분 요청으로 병리학적 검사 진행  O)  혈액검사 상...  others   \n",
       "\n",
       "       label_id  \n",
       "13360        17  \n",
       "13362         5  \n",
       "13363         2  \n",
       "13364        17  \n",
       "13365        18  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8660, 11)\n",
      "(963, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df,test_size=0.1, random_state=100 )\n",
    "train.to_csv('files3/train.csv', index=None)\n",
    "test.to_csv('files3/test.csv', index=None)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "(8660, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_dir = 'files'\n",
    "df = pd.read_csv(file_dir + \"/train.csv\")\n",
    "Num_Class = len(df.label.value_counts())\n",
    "print(Num_Class)\n",
    "df.head()\n",
    "print(df.shape)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../ALBERT')\n",
    "import tensorflow as tf\n",
    "import modeling\n",
    "import tokenization\n",
    "import optimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tf.enable_eager_execution()\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 250 #95 percentile of validated top relevant text length is 50\n",
    "BATCH_SIZE = 16\n",
    "MAX_GRAD_NORM = 1.0\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_WARMUP_STEPS= 10000\n",
    "OUTPUT_DIR = \"finetune1\"\n",
    "USE_TPU = False\n",
    "num_class = Num_Class\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "label_list = [i for i in range(num_class)]\n",
    "init_checkpoint=\"models/\"\n",
    "num_train_steps = 100000 \n",
    "run_config = tf.contrib.tpu.RunConfig(model_dir=OUTPUT_DIR, keep_checkpoint_max=60, save_checkpoints_secs=900,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안', '##녕', '반', '##가', '##워', '내', '##이', '##름', '##은', '범', '##곤', ',', '나는', '라', '##면', '##을', '좋아', '##해', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "config = modeling.AlbertConfig.from_json_file(\"albert_config.json\")\n",
    "tokenizer = tokenization.FullTokenizer.from_scratch(vocab_file=\"vocab.txt\", do_lower_case=False, spm_model_file=None)\n",
    "# Test on tokenizer\n",
    "print(tokenizer.tokenize(\"안녕 반가워 내이름은 범곤, 나는 라면을 좋아해 !! \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_input(file, mode, tokenizer):\n",
    "    df_data = pd.read_csv(file)\n",
    "    df_data = df_data[['SE', 'label_id']]\n",
    "#     print(df.columns)\n",
    "#     df_data = pd.read_csv(file, names = ['SE','label_id'], skiprows =1)\n",
    "    df_data = df_data.dropna(subset=['SE'])\n",
    "\n",
    "    if ((mode == tf.estimator.ModeKeys.TRAIN)|(mode == tf.estimator.ModeKeys.EVAL)):    \n",
    "\n",
    "        df_data['text_tokens'] = df_data.SE.apply(tokenizer.tokenize)\n",
    "        print(([df_data.text_tokens]))\n",
    "#         print(len(df_data.text_tokens))\n",
    "\n",
    "#         input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(list(df_data.text_tokens))],\n",
    "#                           maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "        input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in df_data['text_tokens']],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "        \n",
    "        labels = df_data.label_id\n",
    "        # create the mask to ignore the padded elements in the sequences.\n",
    "        input_mask = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "        return input_ids, labels, input_mask\n",
    "\n",
    "    else: # Predict \n",
    "        df_data['text_tokens'] = df_data.SE.apply(tokenizer.tokenize)\n",
    "        input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in df_data['text_tokens']],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "        # create the mask to ignore the padded elements in the sequences.\n",
    "        input_mask = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "        #print(df_data['text_tokens'])\n",
    "        #print(input_ids)\n",
    "        return input_ids, None, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(file, seq_length, drop_remainder, tokenizer):\n",
    "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "    def input_fn(mode, params):\n",
    "        print(\"mode:\", mode)\n",
    "        \n",
    "        input_ids, labels, input_mask = process_input(file, mode, tokenizer)\n",
    "        print(\"input_ids : \", input_ids[0:5] )\n",
    "#         print(\"labels : \", labels )\n",
    "#         print(\"input_mask : \", input_mask[0:5] )\n",
    "    \n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "\n",
    "        num_examples = len(input_ids)\n",
    "        print(\"******num_examples******\", num_examples)\n",
    "\n",
    "        # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "        # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "        # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "        if ((mode == tf.estimator.ModeKeys.TRAIN)|(mode == tf.estimator.ModeKeys.EVAL)):\n",
    "#             d = tf.data.Dataset.from_tensor_slices(\n",
    "#                 ({\"input_ids\": tf.constant( input_ids, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "#                   \"input_mask\": tf.constant( input_mask, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "#                   \"segment_ids\": tf.zeros(shape=[num_examples, seq_length], dtype=tf.int32),}\n",
    "#                  ,tf.constant(labels, shape=[num_examples, seq_length], dtype=tf.int32),))\n",
    "            d = tf.data.Dataset.from_tensor_slices(\n",
    "                ({\"input_ids\": tf.constant( input_ids, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "                  \"input_mask\": tf.constant( input_mask, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "                  \"segment_ids\": tf.zeros(shape=[num_examples, seq_length], dtype=tf.int32),}\n",
    "                 ,tf.constant(labels, shape=[num_examples], dtype=tf.int32),))            \n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                d = d.repeat()\n",
    "                d = d.shuffle(buffer_size=100)\n",
    "\n",
    "            d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "            return d\n",
    "        else: # Predict\n",
    "#             d = tf.data.Dataset.from_tensor_slices(\n",
    "#                 ({\"input_ids\": tf.constant( input_ids, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "#                   \"input_mask\": tf.constant( input_mask, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "#                   \"segment_ids\": tf.zeros(shape=[num_examples, seq_length], dtype=tf.int32),}\n",
    "#                  ,tf.zeros(shape=[num_examples, seq_length], dtype=tf.int32),))\n",
    "            d = tf.data.Dataset.from_tensor_slices(\n",
    "                ({\"input_ids\": tf.constant( input_ids, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "                  \"input_mask\": tf.constant( input_mask, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "                  \"segment_ids\": tf.zeros(shape=[num_examples, seq_length], dtype=tf.int32),}\n",
    "                 ,tf.zeros(shape=[num_examples], dtype=tf.int32),))            \n",
    "\n",
    "            d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "            return d        \n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(albert_config, mode, input_ids, input_mask, segment_ids,\n",
    "                                 labels, num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    model = modeling.AlbertModel(\n",
    "            config=albert_config,\n",
    "            is_training=is_training,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            token_type_ids=segment_ids)\n",
    "\n",
    "    #output_layer = model.get_sequence_output()\n",
    "    output_layer = model.get_pooled_output()\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    print(\"*************************num_labels***********\", num_labels)\n",
    "\n",
    "    output_weight = tf.get_variable(\n",
    "            \"output_weights\", [num_labels, hidden_size],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "            \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "        output_layer = tf.reshape(output_layer, [-1, hidden_size])\n",
    "        logits = tf.matmul(output_layer, output_weight, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        #logits = tf.reshape(logits, [-1, MAX_LEN, num_labels])\n",
    "        logits = tf.reshape(logits, [-1, num_labels])\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        predict = tf.argmax(probabilities,axis=-1)        \n",
    "        if ((mode == tf.estimator.ModeKeys.TRAIN)|(mode == tf.estimator.ModeKeys.EVAL)):\n",
    "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "            one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "            per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "            loss = tf.reduce_sum(per_example_loss)\n",
    "            return (loss, per_example_loss, logits,predict)\n",
    "        else: # Predict\n",
    "            return (None, None, logits,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(albert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     hub_module=None, optimizer=\"adamw\"):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):\t# pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        tf.logging.info(\"*** Features ***\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(f\"\tname = {name}, shape = {features[name].shape}\")\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = labels\n",
    "\n",
    "        (total_loss, per_example_loss, logits, predictions) = \\\n",
    "                create_model(albert_config, mode, input_ids, input_mask,\n",
    "                                         segment_ids, label_ids, num_labels)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        initialized_variable_names = {}\n",
    "        scaffold_fn = None\n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names\n",
    "            ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "            if use_tpu:\n",
    "                def tpu_scaffold():\n",
    "                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "                    return tf.train.Scaffold()\n",
    "\n",
    "                scaffold_fn = tpu_scaffold\n",
    "            else:\n",
    "                print(\"********weights is initialize from checkpoint**********\")\n",
    "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "        tf.logging.info(\"**** Trainable Variables ****\")\n",
    "        for var in tvars:\n",
    "            init_string = \"\"\n",
    "            if var.name in initialized_variable_names:\n",
    "                init_string = \", *INIT_FROM_CKPT*\"\n",
    "            tf.logging.info(f\"\tname = {var.name}, shape = {var.shape}{init_string}\")\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "            train_op = optimization.create_optimizer(\n",
    "                    total_loss, learning_rate, num_train_steps, num_warmup_steps,\n",
    "                    use_tpu, optimizer)\n",
    "\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                    mode=mode,\n",
    "                    loss=total_loss,\n",
    "                    train_op=train_op,\n",
    "                    scaffold_fn=scaffold_fn)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            def metric_fn(per_example_loss, label_ids, logits):\n",
    "                predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "                accuracy = tf.metrics.mean(tf.math.equal(label_ids,predictions))\n",
    "                loss = tf.metrics.mean(values=per_example_loss)\n",
    "                #\n",
    "                return {\n",
    "                    \"eval_accuracy\":accuracy,\n",
    "                    \"eval_loss\": loss,\n",
    "                }                \n",
    "            eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                    mode=mode,\n",
    "                    loss=total_loss,\n",
    "                    eval_metrics=eval_metrics,\n",
    "                    scaffold_fn=scaffold_fn)\n",
    "        else: # Predict\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                    mode=mode,\n",
    "                    predictions= predictions,\n",
    "                    scaffold_fn=scaffold_fn)\n",
    "        return output_spec\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fdc50a85840>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'finetune1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 900, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 60, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdc3297d4e0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "USE_TPU = False\n",
    "model_fn = model_fn_builder(\n",
    "        albert_config=config,\n",
    "        num_labels=len(label_list),\n",
    "        init_checkpoint=init_checkpoint,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=NUM_WARMUP_STEPS,\n",
    "        use_tpu=USE_TPU)\n",
    "\n",
    "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "# or GPU.\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "        use_tpu=USE_TPU,\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        train_batch_size=BATCH_SIZE,\n",
    "        eval_batch_size=BATCH_SIZE,\n",
    "        predict_batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info(\"***** Running training *****\")\n",
    "# import sys\n",
    "import time\n",
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "train_input_fn = input_fn_builder(\n",
    "        file = file_dir + \"/train.csv\",\n",
    "        tokenizer = tokenizer,\n",
    "        seq_length=MAX_LEN,\n",
    "        drop_remainder=True)\n",
    "\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = input_fn_builder(\n",
    "        file = file_dir + \"/test.csv\",\n",
    "        tokenizer = tokenizer,\n",
    "        seq_length=MAX_LEN,\n",
    "        drop_remainder=False)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for prediction in estimator.predict(input_fn=predict_input_fn, checkpoint_path= OUTPUT_DIR + '/model.ckpt-50000'):\n",
    "    #print(\"*******prediction*********\", prediction)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "# for prediction in estimator.predict(input_fn=predict_input_fn, checkpoint_path='animal/models/saved/model.ckpt-25258'):\n",
    "#     #print(\"*******prediction*********\", prediction)\n",
    "#     predictions.append(prediction)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 17, 17, 18, 17, 18, 18, 10, 10, 6, 16, 17, 16, 9, 18, 18, 18, 15, 17, 5, 5, 6, 17, 18, 17, 2, 18, 18, 16, 13, 13, 11, 7, 18, 6, 10, 13, 1, 14, 18, 18, 16, 17, 18, 11, 19, 5, 18, 18, 18, 15, 19, 8, 8, 0, 16, 9, 4, 16, 9, 15, 11, 18, 18, 4, 0, 16, 16, 18, 14, 19, 17, 16, 10, 2, 15, 19, 9, 17, 18, 12, 18, 9, 4, 17, 9, 18, 0, 10, 18, 16, 7, 18, 17, 17, 4, 18, 2, 14, 10, 2, 3, 18, 6, 4, 18, 5, 17, 18, 9, 18, 17, 17, 17, 18, 18, 1, 0, 11, 1, 3, 1, 13, 18, 3, 17, 18, 16, 1, 0, 13, 1, 18, 18, 17, 1, 16, 18, 18, 13, 5, 17, 1, 13, 18, 18, 18, 0, 6, 1, 18, 3, 19, 6, 1, 9, 17, 14, 18, 16, 10, 18, 4, 0, 17, 0, 18, 18, 18, 18, 18, 16, 14, 17, 10, 18, 16, 18, 18, 18, 13, 18, 1, 8, 18, 18, 17, 18, 16, 16, 6, 5, 13, 4, 18, 17, 9, 17, 5, 2, 0, 4, 18, 16, 4, 16, 9, 10, 18, 18, 11, 9, 17, 18, 15, 18, 1, 4, 18, 12, 17, 0, 17, 18, 16, 1, 10, 4, 18, 18, 18, 17, 6, 17, 9, 14, 18, 8, 4, 14, 1, 1, 8, 0, 18, 16, 4, 0, 17, 18, 18, 5, 7, 6, 5, 7, 17, 14, 13, 18, 7, 12, 16, 18, 17, 18, 16, 5, 17, 5, 3, 19, 1, 3, 3, 18, 2, 18, 6, 17, 16, 17, 18, 11, 15, 2, 18, 3, 7, 10, 9, 17, 18, 18, 5, 18, 1, 17, 0, 17, 16, 17, 18, 18, 8, 16, 17, 6, 1, 12, 2, 0, 14, 5, 18, 18, 18, 1, 6, 16, 17, 1, 2, 19, 12, 11, 7, 0, 19, 5, 18, 18, 1, 5, 18, 18, 9, 16, 17, 18, 2, 13, 1, 6, 18, 0, 17, 2, 16, 7, 17, 6, 16, 6, 5, 10, 18, 17, 16, 17, 10, 18, 17, 18, 18, 18, 18, 18, 17, 16, 3, 18, 4, 14, 18, 15, 10, 17, 18, 16, 6, 2, 10, 18, 18, 3, 18, 6, 18, 18, 2, 9, 18, 7, 17, 5, 17, 17, 0, 5, 6, 16, 18, 5, 2, 11, 17, 8, 0, 18, 18, 0, 15, 6, 6, 4, 1, 14, 13, 18, 17, 16, 18, 5, 18, 8, 16, 4, 18, 17, 16, 10, 4, 18, 7, 0, 18, 7, 0, 6, 18, 18, 14, 7, 1, 10, 14, 0, 9, 18, 12, 18, 9, 13, 17, 18, 2, 16, 18, 6, 18, 18, 19, 17, 18, 14, 18, 17, 18, 18, 18, 13, 17, 6, 5, 19, 17, 17, 19, 2, 17, 5, 2, 18, 17, 7, 16, 7, 18, 18, 8, 1, 17, 16, 9, 14, 18, 11, 5, 16, 17, 18, 4, 19, 4, 18, 2, 18, 15, 7, 16, 17, 9, 19, 2, 8, 12, 5, 10, 9, 18, 11, 2, 16, 13, 18, 5, 17, 17, 17, 9, 2, 17, 5, 16, 17, 10, 18, 18, 10, 5, 1, 8, 6, 6, 16, 17, 17, 7, 18, 15, 17, 2, 16, 18, 18, 17, 18, 2, 2, 16, 5, 18, 18, 18, 18, 6, 2, 9, 17, 18, 18, 18, 18, 5, 18, 16, 17, 17, 17, 1, 17, 19, 5, 18, 18, 0, 8, 17, 18, 18, 15, 17, 17, 17, 17, 18, 0, 1, 6, 18, 18, 2, 18, 18, 17, 18, 19, 5, 17, 13, 6, 18, 17, 16, 3, 18, 2, 6, 5, 17, 0, 7, 9, 18, 19, 6, 1, 18, 11, 18, 9, 18, 13, 17, 16, 18, 18, 1, 18, 6, 0, 1, 18, 16, 18, 18, 17, 18, 3, 18, 17, 17, 11, 16, 0, 1, 6, 18, 5, 3, 10, 18, 5, 18, 1, 5, 18, 18, 18, 5, 1, 0, 16, 9, 18, 18, 4, 5, 16, 16, 16, 17, 18, 18, 11, 18, 18, 2, 13, 18, 18, 12, 18, 18, 4, 18, 17, 18, 2, 17, 6, 17, 7, 5, 4, 16, 18, 11, 17, 17, 13, 10, 18, 18, 7, 4, 2, 18, 0, 6, 2, 16, 15, 18, 6, 18, 18, 2, 17, 18, 18, 16, 17, 17, 18, 18, 17, 2, 17, 18, 18, 13, 17, 16, 17, 4, 19, 17, 18, 7, 9, 18, 1, 16, 17, 18, 18, 18, 17, 1, 12, 5, 9, 16, 17, 18, 18, 18, 18, 17, 12, 16, 18, 18, 9, 5, 6, 4, 18, 17, 6, 1, 10, 1, 17, 14, 18, 2, 9, 18, 5, 18, 2, 1, 15, 18, 18, 19, 1, 9, 18, 17, 6, 2, 18, 18, 16, 0, 2, 2, 18, 0, 18, 18, 12, 16, 0, 17, 18, 3, 18, 18, 10, 9, 17, 16, 18, 0, 6, 18, 18, 7, 17, 2, 18, 18, 18, 17, 18, 2, 7, 18, 18, 6, 5, 16, 9, 17, 17, 17, 17, 5, 6, 17, 2, 18, 17, 6, 13, 17, 0, 1, 18, 9, 18, 17, 17, 9, 9, 2, 18, 17, 17, 16, 16, 6, 18, 4, 14, 17, 18, 17, 13, 18, 10, 1, 17, 13, 17, 15, 5, 18, 18, 5, 18, 9, 18, 19, 6, 18, 16, 18, 14, 17, 18, 18, 0, 18, 18, 16, 8, 2, 10, 18, 9, 18, 18, 17, 5, 16, 4, 18, 16, 18, 5, 18, 18, 18, 9, 10, 15, 5, 18, 17, 16, 17, 18, 9, 18, 12, 8, 2, 18, 5, 18, 13, 4, 16, 11, 2, 7, 16, 18, 18, 18, 18, 18, 13, 18, 17, 17]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n",
      "[18, 2, 18, 17, 18, 17, 18, 18, 9, 18, 18, 16, 17, 13, 9, 18, 19, 17, 18, 17, 6, 5, 5, 18, 9, 17, 2, 18, 12, 17, 18, 13, 18, 18, 3, 16, 10, 13, 17, 2, 18, 18, 16, 17, 17, 18, 19, 16, 18, 6, 18, 18, 19, 7, 7, 0, 16, 5, 18, 18, 9, 18, 18, 18, 5, 1, 18, 16, 16, 18, 14, 18, 4, 16, 10, 2, 18, 10, 18, 17, 1, 18, 18, 9, 4, 17, 9, 18, 18, 10, 18, 16, 7, 18, 17, 7, 3, 18, 2, 19, 13, 1, 4, 10, 18, 16, 12, 18, 17, 18, 9, 18, 3, 5, 17, 0, 18, 1, 18, 11, 3, 3, 1, 18, 18, 17, 18, 18, 16, 2, 10, 13, 5, 18, 18, 18, 2, 16, 18, 18, 13, 5, 18, 2, 13, 18, 18, 18, 6, 18, 1, 18, 4, 18, 6, 2, 9, 12, 12, 18, 16, 10, 13, 18, 0, 18, 5, 18, 17, 18, 2, 18, 16, 17, 17, 10, 18, 16, 18, 18, 2, 19, 18, 1, 18, 18, 18, 18, 6, 1, 16, 5, 4, 13, 5, 2, 17, 9, 17, 18, 2, 18, 2, 18, 1, 2, 16, 9, 13, 18, 18, 18, 9, 17, 18, 15, 8, 18, 1, 5, 0, 17, 18, 4, 18, 16, 18, 10, 4, 18, 15, 18, 18, 4, 17, 9, 14, 18, 18, 18, 18, 16, 1, 8, 5, 18, 16, 2, 0, 17, 18, 2, 5, 17, 6, 18, 7, 1, 18, 18, 17, 7, 9, 16, 1, 18, 17, 16, 18, 17, 5, 1, 16, 2, 5, 1, 18, 2, 18, 18, 17, 16, 17, 17, 18, 15, 1, 18, 1, 7, 18, 9, 8, 18, 8, 1, 5, 18, 6, 4, 17, 16, 18, 18, 18, 18, 18, 18, 6, 1, 18, 2, 18, 2, 6, 18, 1, 18, 17, 11, 16, 18, 1, 2, 18, 12, 2, 18, 0, 19, 5, 18, 16, 1, 18, 18, 0, 9, 16, 1, 17, 2, 10, 5, 9, 18, 18, 17, 2, 16, 7, 17, 16, 2, 6, 5, 10, 9, 17, 16, 2, 12, 18, 17, 18, 18, 18, 18, 18, 17, 16, 6, 18, 6, 1, 18, 18, 18, 19, 18, 16, 5, 18, 10, 18, 18, 1, 5, 19, 18, 18, 2, 9, 18, 7, 17, 5, 17, 18, 0, 4, 6, 16, 18, 5, 2, 11, 5, 3, 0, 18, 18, 11, 6, 6, 4, 3, 1, 18, 18, 18, 14, 16, 18, 5, 2, 18, 1, 2, 18, 17, 16, 10, 2, 5, 18, 11, 14, 7, 18, 19, 18, 18, 14, 7, 19, 12, 14, 17, 9, 18, 18, 2, 18, 18, 18, 0, 2, 17, 8, 18, 18, 18, 18, 18, 18, 14, 18, 17, 10, 18, 15, 13, 17, 6, 18, 18, 17, 17, 19, 2, 17, 10, 2, 0, 18, 8, 16, 2, 18, 18, 18, 5, 17, 15, 9, 4, 18, 9, 1, 18, 13, 18, 1, 18, 18, 18, 18, 18, 18, 18, 16, 17, 9, 18, 1, 9, 18, 18, 10, 9, 18, 11, 18, 16, 13, 18, 18, 17, 17, 18, 9, 2, 18, 18, 16, 18, 10, 18, 17, 10, 1, 4, 8, 18, 6, 16, 18, 18, 7, 18, 15, 17, 2, 16, 18, 0, 18, 18, 2, 2, 16, 5, 18, 3, 0, 18, 6, 2, 9, 1, 5, 12, 18, 16, 5, 18, 16, 1, 1, 17, 1, 17, 18, 17, 18, 18, 19, 8, 2, 5, 1, 18, 6, 17, 17, 17, 12, 18, 18, 4, 1, 18, 2, 0, 18, 17, 18, 18, 5, 17, 13, 6, 18, 16, 16, 5, 18, 18, 10, 5, 18, 0, 18, 9, 18, 18, 6, 1, 18, 12, 18, 17, 2, 13, 1, 17, 6, 6, 1, 0, 1, 6, 1, 18, 16, 17, 16, 18, 18, 18, 18, 17, 18, 18, 17, 0, 1, 6, 18, 5, 3, 10, 18, 6, 11, 18, 6, 18, 17, 5, 5, 1, 18, 16, 12, 1, 18, 1, 5, 18, 16, 16, 18, 2, 18, 1, 5, 2, 2, 18, 12, 1, 2, 18, 14, 5, 18, 17, 18, 2, 18, 18, 18, 7, 6, 1, 16, 9, 18, 17, 17, 18, 18, 5, 7, 18, 11, 2, 18, 16, 18, 2, 16, 15, 16, 18, 5, 16, 2, 17, 18, 5, 9, 17, 17, 18, 18, 18, 2, 18, 18, 18, 17, 17, 16, 17, 4, 2, 18, 18, 18, 18, 7, 1, 16, 4, 16, 3, 8, 17, 2, 18, 18, 9, 16, 18, 18, 18, 18, 18, 13, 0, 16, 10, 18, 9, 1, 6, 3, 6, 17, 19, 1, 2, 3, 17, 18, 1, 2, 9, 18, 5, 18, 18, 1, 9, 18, 18, 18, 1, 9, 18, 1, 6, 2, 7, 0, 16, 18, 2, 1, 18, 18, 5, 18, 2, 16, 18, 16, 18, 3, 18, 5, 10, 16, 7, 16, 18, 18, 11, 18, 0, 7, 1, 2, 18, 0, 18, 17, 18, 2, 7, 18, 13, 18, 1, 18, 9, 17, 17, 18, 1, 5, 5, 17, 2, 18, 18, 18, 13, 17, 18, 1, 17, 9, 7, 12, 17, 9, 9, 1, 18, 16, 17, 16, 17, 6, 12, 2, 17, 16, 18, 17, 13, 18, 10, 11, 12, 13, 7, 1, 5, 18, 18, 4, 1, 10, 16, 19, 18, 18, 18, 1, 18, 18, 17, 2, 18, 18, 18, 16, 8, 2, 16, 16, 1, 18, 17, 1, 9, 16, 18, 18, 16, 12, 5, 7, 10, 0, 0, 10, 15, 6, 18, 17, 16, 1, 18, 9, 6, 18, 8, 18, 1, 5, 18, 7, 1, 16, 18, 2, 18, 16, 17, 13, 4, 18, 18, 13, 18, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(file_dir + \"/test.csv\")\n",
    "labels = list(df_test.label_id)\n",
    "print(labels)\n",
    "print(set(labels))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>보호자명</th>\n",
       "      <th>환자명</th>\n",
       "      <th>품종코드</th>\n",
       "      <th>생일</th>\n",
       "      <th>성별코드</th>\n",
       "      <th>검사일</th>\n",
       "      <th>병원</th>\n",
       "      <th>진단코드</th>\n",
       "      <th>SE</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>강경민</td>\n",
       "      <td>마루</td>\n",
       "      <td>508.0</td>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>M</td>\n",
       "      <td>2017-06-05</td>\n",
       "      <td>D</td>\n",
       "      <td>2236</td>\n",
       "      <td>* 384,300원 수납하셨습니다 - 그림      CC : FPV 입원    현증...</td>\n",
       "      <td>others</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>조혜빈(R)</td>\n",
       "      <td>모카</td>\n",
       "      <td>499.0</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>FS</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>F</td>\n",
       "      <td>2046</td>\n",
       "      <td>cc ; 식도이물 서울숲 refer, 위내이물도 의심됨        보호자 돼지뼈를...</td>\n",
       "      <td>2046</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>홍경숙</td>\n",
       "      <td>사랑</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2007-12-03</td>\n",
       "      <td>FS</td>\n",
       "      <td>2017-01-18</td>\n",
       "      <td>E</td>\n",
       "      <td>normal,normal</td>\n",
       "      <td>$-  S)   V- 식욕    활기     백신     사상충           ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유차남R</td>\n",
       "      <td>사랑이</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2012-02-27</td>\n",
       "      <td>F</td>\n",
       "      <td>2014-06-04</td>\n",
       "      <td>A</td>\n",
       "      <td>2185</td>\n",
       "      <td>40일전 분만 - 5마리.  우측 무릎 탈구.</td>\n",
       "      <td>2185</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>박연선</td>\n",
       "      <td>지코</td>\n",
       "      <td>598.0</td>\n",
       "      <td>2017-05-12</td>\n",
       "      <td>MN</td>\n",
       "      <td>2017-11-03</td>\n",
       "      <td>B</td>\n",
       "      <td>test</td>\n",
       "      <td>털이 푸석푸석하고 체중이 2kg 미만임  중성화 수술 보류. FIP Ab hign ...</td>\n",
       "      <td>test</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>정하나</td>\n",
       "      <td>롬멜</td>\n",
       "      <td>537.0</td>\n",
       "      <td>2015-10-11</td>\n",
       "      <td>MN</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>F</td>\n",
       "      <td>normal,normal</td>\n",
       "      <td>글로블린 수치가 높음      -&gt; 염증성 질환이 의심됨    체온  39.4   ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>김미옥</td>\n",
       "      <td>이슬이</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2005-09-26</td>\n",
       "      <td>F</td>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>E</td>\n",
       "      <td>2101,2101</td>\n",
       "      <td>$-  S) 어제부터 설사, 구토. 체온 높은 것 같다고.    V- 식욕    활...</td>\n",
       "      <td>2101</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>이혜진(R)</td>\n",
       "      <td>예동</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>MN</td>\n",
       "      <td>2018-05-16</td>\n",
       "      <td>F</td>\n",
       "      <td>normal,normal</td>\n",
       "      <td>cc : 금일 새벽 소파에서 뛰어내린 후 우측 앞다리 파행  방사선 :  요척골 골...</td>\n",
       "      <td>normal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>김연숙(R)</td>\n",
       "      <td>준</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2006-05-01</td>\n",
       "      <td>M</td>\n",
       "      <td>2018-03-13</td>\n",
       "      <td>F</td>\n",
       "      <td>normal,normal</td>\n",
       "      <td>cc : 아지병원에서 피하잠복고환 확인 후 refer, 평소 흡기시 코골이  방사선...</td>\n",
       "      <td>normal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>김민정</td>\n",
       "      <td>오순이</td>\n",
       "      <td>598.0</td>\n",
       "      <td>2016-10-17</td>\n",
       "      <td>FS</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>F</td>\n",
       "      <td>normal,normal</td>\n",
       "      <td>Hx: 최근 발정 증상, 밤에 증상이 심해서 민원들어옴  PE: BAR  항체가 검...</td>\n",
       "      <td>normal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     보호자명  환자명   품종코드          생일              성별코드         검사일 병원  \\\n",
       "0     강경민   마루  508.0  2016-12-04                 M  2017-06-05  D   \n",
       "1  조혜빈(R)   모카  499.0  2016-08-16  FS                2017-08-16  F   \n",
       "2     홍경숙   사랑  125.0  2007-12-03  FS                2017-01-18  E   \n",
       "3    유차남R  사랑이  123.0  2012-02-27                 F  2014-06-04  A   \n",
       "4     박연선   지코  598.0  2017-05-12                MN  2017-11-03  B   \n",
       "5     정하나   롬멜  537.0  2015-10-11  MN                2017-07-20  F   \n",
       "6     김미옥  이슬이  131.0  2005-09-26  F                 2016-04-04  E   \n",
       "7  이혜진(R)   예동  119.0  2017-05-16  MN                2018-05-16  F   \n",
       "8  김연숙(R)    준  125.0  2006-05-01  M                 2018-03-13  F   \n",
       "9     김민정  오순이  598.0  2016-10-17  FS                2017-08-20  F   \n",
       "\n",
       "            진단코드                                                 SE   label  \\\n",
       "0           2236  * 384,300원 수납하셨습니다 - 그림      CC : FPV 입원    현증...  others   \n",
       "1           2046  cc ; 식도이물 서울숲 refer, 위내이물도 의심됨        보호자 돼지뼈를...    2046   \n",
       "2  normal,normal  $-  S)   V- 식욕    활기     백신     사상충           ...  normal   \n",
       "3           2185                       40일전 분만 - 5마리.  우측 무릎 탈구.       2185   \n",
       "4           test  털이 푸석푸석하고 체중이 2kg 미만임  중성화 수술 보류. FIP Ab hign ...    test   \n",
       "5  normal,normal  글로블린 수치가 높음      -> 염증성 질환이 의심됨    체온  39.4   ...  normal   \n",
       "6      2101,2101  $-  S) 어제부터 설사, 구토. 체온 높은 것 같다고.    V- 식욕    활...    2101   \n",
       "7  normal,normal  cc : 금일 새벽 소파에서 뛰어내린 후 우측 앞다리 파행  방사선 :  요척골 골...  normal   \n",
       "8  normal,normal  cc : 아지병원에서 피하잠복고환 확인 후 refer, 평소 흡기시 코골이  방사선...  normal   \n",
       "9  normal,normal  Hx: 최근 발정 증상, 밤에 증상이 심해서 민원들어옴  PE: BAR  항체가 검...  normal   \n",
       "\n",
       "   label_id  \n",
       "0        18  \n",
       "1         2  \n",
       "2        17  \n",
       "3        14  \n",
       "4        19  \n",
       "5        17  \n",
       "6         9  \n",
       "7        17  \n",
       "8        17  \n",
       "9        17  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(file_dir + \"/train.csv\")\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.514018691588785\n",
      "wrong 468 right 495 total 963\n"
     ]
    }
   ],
   "source": [
    "right = 0\n",
    "wrong = 0\n",
    "result_df = pd.DataFrame(columns=['label', 'predict'])\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == predictions[i] :\n",
    "        right += 1\n",
    "    else :\n",
    "        wrong += 1\n",
    "#         print( \"Wrong label : {} predict {} \".format(labels[i], predictions[i]) )\n",
    "    result_df.loc[i,'label'] = labels[i]\n",
    "    result_df.loc[i,'predict'] = predictions[i]\n",
    "\n",
    "accuracy = right/(right+wrong)\n",
    "print(\"accuracy :\", accuracy)\n",
    "print(\"wrong {} right {} total {}\".format(wrong, right, len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : 16\n",
      "16    58\n",
      "18     6\n",
      "17     5\n",
      "1      3\n",
      "15     1\n",
      "13     1\n",
      "9      1\n",
      "2      1\n",
      "Name: predict, dtype: int64\n",
      "0.7631578947368421\n"
     ]
    }
   ],
   "source": [
    "LABEL = 16\n",
    "print('Label :', LABEL)\n",
    "print((result_df[result_df['label']==LABEL]).predict.value_counts())\n",
    "temp_df = result_df[result_df['label']==LABEL]\n",
    "print(len(temp_df[temp_df['predict']==LABEL])/len(temp_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     20\n",
       "2      6\n",
       "18     5\n",
       "5      3\n",
       "17     2\n",
       "3      2\n",
       "19     1\n",
       "11     1\n",
       "4      1\n",
       "16     1\n",
       "Name: predict, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result_df[result_df['label']==1]).predict.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "predict_input_fn = input_fn_builder(\n",
    "        file = file_dir + \"/test.csv\",\n",
    "        tokenizer = tokenizer,\n",
    "        seq_length=MAX_LEN,\n",
    "        drop_remainder=False)\n",
    "\n",
    "df_test = pd.read_csv(file_dir + \"/test.csv\")\n",
    "labels = list(df_test.label_id)\n",
    "# print(labels)\n",
    "print(set(labels))\n",
    "predictions = []\n",
    "\n",
    "def make_prob (labels, predictions) :\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i] :\n",
    "            right += 1\n",
    "        else :\n",
    "            wrong += 1\n",
    "    accuracy = right/(right+wrong)\n",
    "    return accuracy\n",
    "            \n",
    "    #         print( \"Wrong label : {} predict {} \".format(labels[i], predictions[i]) )\n",
    "\n",
    "saved_ckpts = [ckpt for ckpt in os.listdir(OUTPUT_DIR + '/') if ckpt.endswith('.index')] \n",
    "accuracy_df = pd.DataFrame(columns=['ckpt', 'accuracy'])\n",
    "# print(saved_ckpts)\n",
    "\n",
    "index = 0\n",
    "for ckpt in saved_ckpts :\n",
    "    ckpt = ckpt[:-6]\n",
    "    print(ckpt)\n",
    "    predictions = []\n",
    "    for prediction in estimator.predict(input_fn=predict_input_fn, checkpoint_path=OUTPUT_DIR+'/' + ckpt):\n",
    "        predictions.append(prediction)\n",
    "    accuracy = make_prob(labels, predictions)\n",
    "    accuracy_df.loc[index, 'ckpt'] = ckpt\n",
    "    accuracy_df.loc[index, 'accuracy'] = accuracy\n",
    "    index += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ckpt</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.ckpt-60012</td>\n",
       "      <td>0.481828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model.ckpt-86755</td>\n",
       "      <td>0.464174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model.ckpt-11045</td>\n",
       "      <td>0.49325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model.ckpt-93444</td>\n",
       "      <td>0.49325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.ckpt-88984</td>\n",
       "      <td>0.46729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model.ckpt-31044</td>\n",
       "      <td>0.469367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model.ckpt-33272</td>\n",
       "      <td>0.460021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model.ckpt-19905</td>\n",
       "      <td>0.491173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model.ckpt-75611</td>\n",
       "      <td>0.499481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model.ckpt-82296</td>\n",
       "      <td>0.476636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model.ckpt-53325</td>\n",
       "      <td>0.501558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model.ckpt-73383</td>\n",
       "      <td>0.475597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model.ckpt-6630</td>\n",
       "      <td>0.454829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model.ckpt-0</td>\n",
       "      <td>0.07892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model.ckpt-66697</td>\n",
       "      <td>0.501558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>model.ckpt-42184</td>\n",
       "      <td>0.507788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>model.ckpt-48868</td>\n",
       "      <td>0.426791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>model.ckpt-2950</td>\n",
       "      <td>0.500519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>model.ckpt-35500</td>\n",
       "      <td>0.475597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>model.ckpt-22133</td>\n",
       "      <td>0.498442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>model.ckpt-28816</td>\n",
       "      <td>0.507788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>model.ckpt-57783</td>\n",
       "      <td>0.460021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model.ckpt-4425</td>\n",
       "      <td>0.472482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>model.ckpt-62240</td>\n",
       "      <td>0.475597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>model.ckpt-1474</td>\n",
       "      <td>0.4081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>model.ckpt-77840</td>\n",
       "      <td>0.478712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>model.ckpt-46640</td>\n",
       "      <td>0.496366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>model.ckpt-26588</td>\n",
       "      <td>0.490135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>model.ckpt-68926</td>\n",
       "      <td>0.48702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>model.ckpt-95674</td>\n",
       "      <td>0.489097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>model.ckpt-39956</td>\n",
       "      <td>0.508827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>model.ckpt-64468</td>\n",
       "      <td>0.478712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>model.ckpt-13240</td>\n",
       "      <td>0.495327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>model.ckpt-84525</td>\n",
       "      <td>0.481828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>model.ckpt-37728</td>\n",
       "      <td>0.496366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>model.ckpt-91214</td>\n",
       "      <td>0.48702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>model.ckpt-17677</td>\n",
       "      <td>0.475597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>model.ckpt-80068</td>\n",
       "      <td>0.491173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>model.ckpt-100000</td>\n",
       "      <td>0.488058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>model.ckpt-8833</td>\n",
       "      <td>0.475597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>model.ckpt-97904</td>\n",
       "      <td>0.490135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>model.ckpt-15454</td>\n",
       "      <td>0.488058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>model.ckpt-44412</td>\n",
       "      <td>0.502596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>model.ckpt-24361</td>\n",
       "      <td>0.490135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>model.ckpt-55554</td>\n",
       "      <td>0.462098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>model.ckpt-71154</td>\n",
       "      <td>0.47352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>model.ckpt-51096</td>\n",
       "      <td>0.49325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ckpt  accuracy\n",
       "0    model.ckpt-60012  0.481828\n",
       "1    model.ckpt-86755  0.464174\n",
       "2    model.ckpt-11045   0.49325\n",
       "3    model.ckpt-93444   0.49325\n",
       "4    model.ckpt-88984   0.46729\n",
       "5    model.ckpt-31044  0.469367\n",
       "6    model.ckpt-33272  0.460021\n",
       "7    model.ckpt-19905  0.491173\n",
       "8    model.ckpt-75611  0.499481\n",
       "9    model.ckpt-82296  0.476636\n",
       "10   model.ckpt-53325  0.501558\n",
       "11   model.ckpt-73383  0.475597\n",
       "12    model.ckpt-6630  0.454829\n",
       "13       model.ckpt-0   0.07892\n",
       "14   model.ckpt-66697  0.501558\n",
       "15   model.ckpt-42184  0.507788\n",
       "16   model.ckpt-48868  0.426791\n",
       "17    model.ckpt-2950  0.500519\n",
       "18   model.ckpt-35500  0.475597\n",
       "19   model.ckpt-22133  0.498442\n",
       "20   model.ckpt-28816  0.507788\n",
       "21   model.ckpt-57783  0.460021\n",
       "22    model.ckpt-4425  0.472482\n",
       "23   model.ckpt-62240  0.475597\n",
       "24    model.ckpt-1474    0.4081\n",
       "25   model.ckpt-77840  0.478712\n",
       "26   model.ckpt-46640  0.496366\n",
       "27   model.ckpt-26588  0.490135\n",
       "28   model.ckpt-68926   0.48702\n",
       "29   model.ckpt-95674  0.489097\n",
       "30   model.ckpt-39956  0.508827\n",
       "31   model.ckpt-64468  0.478712\n",
       "32   model.ckpt-13240  0.495327\n",
       "33   model.ckpt-84525  0.481828\n",
       "34   model.ckpt-37728  0.496366\n",
       "35   model.ckpt-91214   0.48702\n",
       "36   model.ckpt-17677  0.475597\n",
       "37   model.ckpt-80068  0.491173\n",
       "38  model.ckpt-100000  0.488058\n",
       "39    model.ckpt-8833  0.475597\n",
       "40   model.ckpt-97904  0.490135\n",
       "41   model.ckpt-15454  0.488058\n",
       "42   model.ckpt-44412  0.502596\n",
       "43   model.ckpt-24361  0.490135\n",
       "44   model.ckpt-55554  0.462098\n",
       "45   model.ckpt-71154   0.47352\n",
       "46   model.ckpt-51096   0.49325"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ckpt</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model.ckpt-0</td>\n",
       "      <td>0.07892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>model.ckpt-1474</td>\n",
       "      <td>0.4081</td>\n",
       "      <td>1474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>model.ckpt-2950</td>\n",
       "      <td>0.500519</td>\n",
       "      <td>2950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model.ckpt-4425</td>\n",
       "      <td>0.472482</td>\n",
       "      <td>4425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model.ckpt-6630</td>\n",
       "      <td>0.454829</td>\n",
       "      <td>6630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>model.ckpt-8833</td>\n",
       "      <td>0.475597</td>\n",
       "      <td>8833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model.ckpt-11045</td>\n",
       "      <td>0.49325</td>\n",
       "      <td>11045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>model.ckpt-13240</td>\n",
       "      <td>0.495327</td>\n",
       "      <td>13240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>model.ckpt-15454</td>\n",
       "      <td>0.488058</td>\n",
       "      <td>15454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>model.ckpt-17677</td>\n",
       "      <td>0.475597</td>\n",
       "      <td>17677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model.ckpt-19905</td>\n",
       "      <td>0.491173</td>\n",
       "      <td>19905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>model.ckpt-22133</td>\n",
       "      <td>0.498442</td>\n",
       "      <td>22133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>model.ckpt-24361</td>\n",
       "      <td>0.490135</td>\n",
       "      <td>24361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>model.ckpt-26588</td>\n",
       "      <td>0.490135</td>\n",
       "      <td>26588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>model.ckpt-28816</td>\n",
       "      <td>0.507788</td>\n",
       "      <td>28816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model.ckpt-31044</td>\n",
       "      <td>0.469367</td>\n",
       "      <td>31044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model.ckpt-33272</td>\n",
       "      <td>0.460021</td>\n",
       "      <td>33272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>model.ckpt-35500</td>\n",
       "      <td>0.475597</td>\n",
       "      <td>35500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>model.ckpt-37728</td>\n",
       "      <td>0.496366</td>\n",
       "      <td>37728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>model.ckpt-39956</td>\n",
       "      <td>0.508827</td>\n",
       "      <td>39956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>model.ckpt-42184</td>\n",
       "      <td>0.507788</td>\n",
       "      <td>42184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>model.ckpt-44412</td>\n",
       "      <td>0.502596</td>\n",
       "      <td>44412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>model.ckpt-46640</td>\n",
       "      <td>0.496366</td>\n",
       "      <td>46640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>model.ckpt-48868</td>\n",
       "      <td>0.426791</td>\n",
       "      <td>48868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>model.ckpt-51096</td>\n",
       "      <td>0.49325</td>\n",
       "      <td>51096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model.ckpt-53325</td>\n",
       "      <td>0.501558</td>\n",
       "      <td>53325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>model.ckpt-55554</td>\n",
       "      <td>0.462098</td>\n",
       "      <td>55554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>model.ckpt-57783</td>\n",
       "      <td>0.460021</td>\n",
       "      <td>57783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.ckpt-60012</td>\n",
       "      <td>0.481828</td>\n",
       "      <td>60012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>model.ckpt-62240</td>\n",
       "      <td>0.475597</td>\n",
       "      <td>62240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>model.ckpt-64468</td>\n",
       "      <td>0.478712</td>\n",
       "      <td>64468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model.ckpt-66697</td>\n",
       "      <td>0.501558</td>\n",
       "      <td>66697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>model.ckpt-68926</td>\n",
       "      <td>0.48702</td>\n",
       "      <td>68926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>model.ckpt-71154</td>\n",
       "      <td>0.47352</td>\n",
       "      <td>71154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model.ckpt-73383</td>\n",
       "      <td>0.475597</td>\n",
       "      <td>73383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model.ckpt-75611</td>\n",
       "      <td>0.499481</td>\n",
       "      <td>75611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>model.ckpt-77840</td>\n",
       "      <td>0.478712</td>\n",
       "      <td>77840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>model.ckpt-80068</td>\n",
       "      <td>0.491173</td>\n",
       "      <td>80068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model.ckpt-82296</td>\n",
       "      <td>0.476636</td>\n",
       "      <td>82296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>model.ckpt-84525</td>\n",
       "      <td>0.481828</td>\n",
       "      <td>84525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model.ckpt-86755</td>\n",
       "      <td>0.464174</td>\n",
       "      <td>86755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.ckpt-88984</td>\n",
       "      <td>0.46729</td>\n",
       "      <td>88984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>model.ckpt-91214</td>\n",
       "      <td>0.48702</td>\n",
       "      <td>91214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model.ckpt-93444</td>\n",
       "      <td>0.49325</td>\n",
       "      <td>93444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>model.ckpt-95674</td>\n",
       "      <td>0.489097</td>\n",
       "      <td>95674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>model.ckpt-97904</td>\n",
       "      <td>0.490135</td>\n",
       "      <td>97904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>model.ckpt-100000</td>\n",
       "      <td>0.488058</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ckpt  accuracy   index\n",
       "13       model.ckpt-0   0.07892       0\n",
       "24    model.ckpt-1474    0.4081    1474\n",
       "17    model.ckpt-2950  0.500519    2950\n",
       "22    model.ckpt-4425  0.472482    4425\n",
       "12    model.ckpt-6630  0.454829    6630\n",
       "39    model.ckpt-8833  0.475597    8833\n",
       "2    model.ckpt-11045   0.49325   11045\n",
       "32   model.ckpt-13240  0.495327   13240\n",
       "41   model.ckpt-15454  0.488058   15454\n",
       "36   model.ckpt-17677  0.475597   17677\n",
       "7    model.ckpt-19905  0.491173   19905\n",
       "19   model.ckpt-22133  0.498442   22133\n",
       "43   model.ckpt-24361  0.490135   24361\n",
       "27   model.ckpt-26588  0.490135   26588\n",
       "20   model.ckpt-28816  0.507788   28816\n",
       "5    model.ckpt-31044  0.469367   31044\n",
       "6    model.ckpt-33272  0.460021   33272\n",
       "18   model.ckpt-35500  0.475597   35500\n",
       "34   model.ckpt-37728  0.496366   37728\n",
       "30   model.ckpt-39956  0.508827   39956\n",
       "15   model.ckpt-42184  0.507788   42184\n",
       "42   model.ckpt-44412  0.502596   44412\n",
       "26   model.ckpt-46640  0.496366   46640\n",
       "16   model.ckpt-48868  0.426791   48868\n",
       "46   model.ckpt-51096   0.49325   51096\n",
       "10   model.ckpt-53325  0.501558   53325\n",
       "44   model.ckpt-55554  0.462098   55554\n",
       "21   model.ckpt-57783  0.460021   57783\n",
       "0    model.ckpt-60012  0.481828   60012\n",
       "23   model.ckpt-62240  0.475597   62240\n",
       "31   model.ckpt-64468  0.478712   64468\n",
       "14   model.ckpt-66697  0.501558   66697\n",
       "28   model.ckpt-68926   0.48702   68926\n",
       "45   model.ckpt-71154   0.47352   71154\n",
       "11   model.ckpt-73383  0.475597   73383\n",
       "8    model.ckpt-75611  0.499481   75611\n",
       "25   model.ckpt-77840  0.478712   77840\n",
       "37   model.ckpt-80068  0.491173   80068\n",
       "9    model.ckpt-82296  0.476636   82296\n",
       "33   model.ckpt-84525  0.481828   84525\n",
       "1    model.ckpt-86755  0.464174   86755\n",
       "4    model.ckpt-88984   0.46729   88984\n",
       "35   model.ckpt-91214   0.48702   91214\n",
       "3    model.ckpt-93444   0.49325   93444\n",
       "29   model.ckpt-95674  0.489097   95674\n",
       "40   model.ckpt-97904  0.490135   97904\n",
       "38  model.ckpt-100000  0.488058  100000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df.sort_values(by=['ckpt'], axis=0)\n",
    "accuracy_df['index'] = accuracy_df.ckpt.apply(lambda x : int(str(x).split('-')[1]) )\n",
    "accuracy_df.sort_values(by=['index'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
